---
title: "LangChain Integration"
description: "Use Venice AI with LangChain for chains, agents, and RAG pipelines"
"og:title": "LangChain Integration | Venice API Docs"
"og:description": "Build LangChain applications powered by Venice AI's private, uncensored models"
---

Venice AI works seamlessly with [LangChain](https://python.langchain.com/) thanks to full OpenAI SDK compatibility. Build chains, agents, and RAG pipelines with Venice's privacy-first infrastructure.

## Setup

```bash
pip install langchain langchain-openai openai
```

## Chat Models

Use `ChatOpenAI` with Venice's base URL:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="venice-uncensored",
    api_key="your-venice-api-key",
    base_url="https://api.venice.ai/api/v1",
    temperature=0.7,
)

response = llm.invoke("Explain privacy-preserving AI in 2 sentences.")
print(response.content)
```

## Streaming

```python
for chunk in llm.stream("Write a haiku about decentralization."):
    print(chunk.content, end="", flush=True)
```

## Embeddings

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-bge-m3",
    openai_api_key="your-venice-api-key",
    openai_api_base="https://api.venice.ai/api/v1",
)

vectors = embeddings.embed_documents([
    "Venice AI provides private inference.",
    "No data is retained after processing.",
])
print(f"Embedding dimension: {len(vectors[0])}")
```

## Chains

### Simple Chain with Prompt Template

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a {role}. Answer concisely."),
    ("user", "{question}"),
])

chain = prompt | llm
response = chain.invoke({"role": "privacy expert", "question": "Why does zero data retention matter?"})
print(response.content)
```

### Sequential Chain

```python
from langchain_core.output_parsers import StrOutputParser

# Chain 1: Generate a topic summary
summarizer = ChatPromptTemplate.from_messages([
    ("user", "Summarize this topic in 3 bullet points: {topic}")
]) | llm | StrOutputParser()

# Chain 2: Generate questions from summary
questioner = ChatPromptTemplate.from_messages([
    ("user", "Based on this summary, generate 3 thought-provoking questions:\n{summary}")
]) | llm | StrOutputParser()

# Compose
summary = summarizer.invoke({"topic": "decentralized AI inference"})
questions = questioner.invoke({"summary": summary})
print(questions)
```

## RAG Pipeline

Build a retrieval-augmented generation pipeline with Venice:

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Initialize Venice models
llm = ChatOpenAI(
    model="zai-org-glm-4.7",
    api_key="your-venice-api-key",
    base_url="https://api.venice.ai/api/v1",
)

embeddings = OpenAIEmbeddings(
    model="text-embedding-bge-m3",
    openai_api_key="your-venice-api-key",
    openai_api_base="https://api.venice.ai/api/v1",
)

# Load and split documents
documents = [
    "Venice AI provides private, uncensored AI inference with zero data retention.",
    "The Venice API is OpenAI-compatible, supporting chat completions, images, audio, video, and embeddings.",
    "Venice supports function calling, structured outputs, web search, and reasoning models.",
    "Privacy levels include Private (zero retention) and Anonymized (third-party processed).",
]

# Create vector store
vectorstore = FAISS.from_texts(documents, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# RAG prompt
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the question based only on the following context:\n\n{context}"),
    ("user", "{question}"),
])

# RAG chain
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | rag_prompt
    | llm
    | StrOutputParser()
)

answer = rag_chain.invoke("What privacy levels does Venice offer?")
print(answer)
```

## Function Calling with Agents

```python
from langchain_core.tools import tool
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate

# Use a function-calling capable model
llm = ChatOpenAI(
    model="zai-org-glm-4.7",
    api_key="your-venice-api-key",
    base_url="https://api.venice.ai/api/v1",
)

@tool
def get_venice_model_price(model_id: str) -> str:
    """Get the pricing for a Venice AI model."""
    prices = {
        "venice-uncensored": "Input: $0.20/1M, Output: $0.90/1M",
        "zai-org-glm-4.7": "Input: $0.55/1M, Output: $2.65/1M",
        "qwen3-4b": "Input: $0.05/1M, Output: $0.15/1M",
    }
    return prices.get(model_id, f"Model {model_id} not found in price list.")

prompt = ChatPromptTemplate.from_messages([
    ("system", "You help users find the right Venice AI model. Use tools when needed."),
    ("placeholder", "{chat_history}"),
    ("user", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])

agent = create_tool_calling_agent(llm, [get_venice_model_price], prompt)
executor = AgentExecutor(agent=agent, tools=[get_venice_model_price], verbose=True)

result = executor.invoke({"input": "What's the cheapest Venice text model?", "chat_history": []})
print(result["output"])
```

## Structured Output

```python
from pydantic import BaseModel, Field

class MovieReview(BaseModel):
    title: str = Field(description="Movie title")
    rating: float = Field(description="Rating out of 10")
    summary: str = Field(description="One-sentence summary")

structured_llm = llm.with_structured_output(MovieReview)
review = structured_llm.invoke("Review the movie Inception")
print(f"{review.title}: {review.rating}/10 â€” {review.summary}")
```

## Web Search Integration

Use Venice's built-in web search via `venice_parameters`:

```python
from langchain_openai import ChatOpenAI

llm_with_search = ChatOpenAI(
    model="venice-uncensored",
    api_key="your-venice-api-key",
    base_url="https://api.venice.ai/api/v1",
    model_kwargs={
        "venice_parameters": {
            "enable_web_search": "auto"
        }
    }
)

response = llm_with_search.invoke("What are the latest developments in AI this week?")
print(response.content)
```

## Recommended Models for LangChain

| Use Case | Model | Why |
|----------|-------|-----|
| General chains | `venice-uncensored` | Fast, cheap, uncensored |
| Complex reasoning | `zai-org-glm-4.7` | Best private flagship model |
| Function calling | `zai-org-glm-4.7` | Reliable tool use |
| Vision + text | `mistral-31-24b` | Vision + function calling |
| Code generation | `qwen3-coder-480b-a35b-instruct` | Optimized for code |
| Embeddings (RAG) | `text-embedding-bge-m3` | Private embeddings |
| Budget / high-volume | `qwen3-4b` | $0.05/1M input |

<Card title="View All Models" icon="database" href="/models/overview">
  Browse all Venice models with pricing and capabilities
</Card>

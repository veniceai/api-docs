---
title: "Reasoning Models"
description: "Using reasoning models with visible thinking in the Venice API"
'og:title': "Reasoning Models"
'og:description': "Using reasoning models with visible thinking in the Venice API"
---

Some models think out loud before answering. They work through the problem step by step, then give you a final answer. This makes them better at math, code, and logic-heavy tasks.

Venice models that support reasoning: `qwen3-235b`, `qwen3-235b-a22b-thinking-2507`, `qwen3-4b`, `deepseek-ai-DeepSeek-R1`

## How it works

The model wraps its thinking in `<think>` tags, then gives the answer:

```
<think>
The user wants 15% of 240.
15% = 0.15
0.15 Ã— 240 = 36
</think>

15% of 240 is **36**.
```

You can work with this in three ways:

### Just get the answer

Set `strip_thinking_response` to true and Venice removes the `<think>` block for you:

<CodeGroup>
```python Python
response = client.chat.completions.create(
    model="qwen3-235b",
    messages=[{"role": "user", "content": "What is 15% of 240?"}],
    extra_body={
        "venice_parameters": {
            "strip_thinking_response": True
        }
    }
)

print(response.choices[0].message.content)
# "15% of 240 is **36**."
```

```javascript Node.js
const response = await client.chat.completions.create({
    model: "qwen3-235b",
    messages: [{ role: "user", content: "What is 15% of 240?" }],
    venice_parameters: {
        strip_thinking_response: true
    }
});

console.log(response.choices[0].message.content);
// "15% of 240 is **36**."
```

```bash cURL
curl https://api.venice.ai/api/v1/chat/completions \
  -H "Authorization: Bearer $VENICE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-235b",
    "messages": [{"role": "user", "content": "What is 15% of 240?"}],
    "venice_parameters": {
      "strip_thinking_response": true
    }
  }'
```
</CodeGroup>

Or use a model suffix: `qwen3-235b:strip_thinking_response=true`

### Parse it yourself

If you want both the thinking and the answer, grab the full response and split it:

<CodeGroup>
```python Python
import re

response = client.chat.completions.create(
    model="qwen3-235b",
    messages=[{"role": "user", "content": "What is 15% of 240?"}]
)

content = response.choices[0].message.content

match = re.search(r'<think>(.*?)</think>', content, re.DOTALL)
thinking = match.group(1).strip() if match else None
answer = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
```

```javascript Node.js
const response = await client.chat.completions.create({
    model: "qwen3-235b",
    messages: [{ role: "user", content: "What is 15% of 240?" }]
});

const content = response.choices[0].message.content;

const match = content.match(/<think>([\s\S]*?)<\/think>/);
const thinking = match ? match[1].trim() : null;
const answer = content.replace(/<think>[\s\S]*?<\/think>/, '').trim();
```
</CodeGroup>

### Skip thinking entirely

If you don't need the model to reason (faster, cheaper), disable it:

```python
response = client.chat.completions.create(
    model="qwen3-4b",
    messages=[{"role": "user", "content": "What's the capital of France?"}],
    extra_body={
        "venice_parameters": {
            "disable_thinking": True
        }
    }
)
```

Not all models support this. See the note below about `qwen3-235b`.

## The `reasoning_content` field

`qwen3-235b-a22b-thinking-2507` also puts the thinking in a separate `reasoning_content` field:

```python
response = client.chat.completions.create(
    model="qwen3-235b-a22b-thinking-2507",
    messages=[{"role": "user", "content": "What is 15% of 240?"}]
)

thinking = response.choices[0].message.reasoning_content
```

The `content` field still has `<think>` tags though, so you'll still need to strip them if you want clean output.

## Streaming

When streaming, the `<think>` content comes first. Parse after you've collected the full response:

```python
import re

stream = client.chat.completions.create(
    model="qwen3-235b",
    messages=[{"role": "user", "content": "Explain photosynthesis"}],
    stream=True
)

full = ""
for chunk in stream:
    if chunk.choices[0].delta.content:
        full += chunk.choices[0].delta.content

thinking = re.search(r'<think>(.*?)</think>', full, re.DOTALL)
answer = re.sub(r'<think>.*?</think>', '', full, flags=re.DOTALL).strip()
```

## qwen3-235b deprecation

<Warning>
Starting **December 14, 2025**, `qwen3-235b` routes to `qwen3-235b-a22b-thinking-2507`.

**What changes:**
- `disable_thinking` gets ignored
- `reasoning_content` field becomes available

**What stays the same:**
- `<think>` tags still appear in content
- `strip_thinking_response` still works
- Your existing code keeps working

If you use `disable_thinking=true`, switch to `qwen3-235b-a22b-instruct-2507` before December 14.
</Warning>

## Parameters

| Parameter | What it does |
|-----------|--------------|
| `strip_thinking_response` | Remove `<think>` tags, return only the answer |
| `disable_thinking` | Skip reasoning entirely (faster) |

Both go in `venice_parameters` or as model suffixes.

For pricing and context limits, see [Current Models](/overview/models).

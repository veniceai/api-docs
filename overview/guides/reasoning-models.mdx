---
title: "Reasoning Models"
description: "Using reasoning models with visible thinking in the Venice API"
'og:title': "Reasoning Models | Venice API Docs"
'og:description': "Using reasoning models with visible thinking in the Venice API"
---

Some models think out loud before answering. They work through problems step by step, then give you a final answer. This makes them stronger at math, code, and logic-heavy tasks.

{/* BEGIN_SUPPORTED_MODELS */}
| Model | ID |
|-------|-----|
| Claude Opus 4.5 | `claude-opus-45` |
| Claude Opus 4.6 | `claude-opus-4-6` |
| Claude Sonnet 4.5 | `claude-sonnet-45` |
| Claude Sonnet 4.6 | `claude-sonnet-4-6` |
| DeepSeek V3.2 | `deepseek-v3.2` |
| Gemini 3 Flash Preview | `gemini-3-flash-preview` |
| Gemini 3 Pro Preview | `gemini-3-pro-preview` |
| Gemini 3.1 Pro Preview | `gemini-3-1-pro-preview` |
| GLM 4.7 | `zai-org-glm-4.7` |
| GLM 4.7 Flash | `zai-org-glm-4.7-flash` |
| GLM 4.7 Flash Heretic | `olafangensan-glm-4.7-flash-heretic` |
| GLM 5 | `zai-org-glm-5` |
| GPT-5.2 | `openai-gpt-52` |
| GPT-5.2 Codex | `openai-gpt-52-codex` |
| Grok 4.1 Fast | `grok-41-fast` |
| Grok Code Fast 1 | `grok-code-fast-1` |
| Kimi K2 Thinking | `kimi-k2-thinking` |
| Kimi K2.5 | `kimi-k2-5` |
| MiniMax M2.1 | `minimax-m21` |
| MiniMax M2.5 | `minimax-m25` |
| Qwen 3 235B A22B Thinking 2507 | `qwen3-235b-a22b-thinking-2507` |
| Venice Small | `qwen3-4b` |
{/* END_SUPPORTED_MODELS */}

See the full list of models and more details on the [Models page](/overview/models).

## Reading the output

Reasoning models return their thinking in one of two ways.

### The `reasoning_content` field

Models like `zai-org-glm-4.7` return thinking in a separate `reasoning_content` field, keeping `content` clean:

<CodeGroup>
```python Python
response = client.chat.completions.create(
    model="zai-org-glm-4.7",
    messages=[{"role": "user", "content": "What is 15% of 240?"}]
)

thinking = response.choices[0].message.reasoning_content
answer = response.choices[0].message.content
```

```javascript Node.js
const response = await client.chat.completions.create({
    model: "zai-org-glm-4.7",
    messages: [{ role: "user", content: "What is 15% of 240?" }]
});

const thinking = response.choices[0].message.reasoning_content;
const answer = response.choices[0].message.content;
```

```bash cURL
curl https://api.venice.ai/api/v1/chat/completions \
  -H "Authorization: Bearer $VENICE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "zai-org-glm-4.7",
    "messages": [{"role": "user", "content": "What is 15% of 240?"}]
  }'
```
</CodeGroup>

### `<think>` tags

Other models (`qwen3-4b`, `deepseek-ai-DeepSeek-R1`) wrap thinking in `<think>` tags within the `content` field:

```
<think>
The user wants 15% of 240.
15% = 0.15
0.15 Ã— 240 = 36
</think>

15% of 240 is **36**.
```

Parse or strip as needed, or use `strip_thinking_response` to have Venice remove them server-side.

### Streaming

When streaming, `reasoning_content` arrives in the delta before the final answer:

<CodeGroup>
```python Python
stream = client.chat.completions.create(
    model="zai-org-glm-4.7",
    messages=[{"role": "user", "content": "Explain photosynthesis"}],
    stream=True
)

for chunk in stream:
    if chunk.choices:
        delta = chunk.choices[0].delta
        if delta.reasoning_content:
            print(delta.reasoning_content, end="")
        if delta.content:
            print(delta.content, end="")
```

```javascript Node.js
const stream = await client.chat.completions.create({
    model: "zai-org-glm-4.7",
    messages: [{ role: "user", content: "Explain photosynthesis" }],
    stream: true
});

for await (const chunk of stream) {
    if (chunk.choices?.[0]?.delta) {
        const delta = chunk.choices[0].delta;
        if (delta.reasoning_content) process.stdout.write(delta.reasoning_content);
        if (delta.content) process.stdout.write(delta.content);
    }
}
```

```bash cURL
curl https://api.venice.ai/api/v1/chat/completions \
  -H "Authorization: Bearer $VENICE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "zai-org-glm-4.7",
    "messages": [{"role": "user", "content": "Explain photosynthesis"}],
    "stream": true
  }'
```
</CodeGroup>

For models using `<think>` tags, the thinking streams before the answer. Collect the full response, then parse.

## Reasoning effort

Reasoning models spend tokens "thinking" before they answer. The `reasoning_effort` parameter controls how much thinking the model does.

| Value | Behavior |
|-------|----------|
| `low` | Minimal thinking. Fast and cheap. Best for simple factual questions. |
| `medium` | Balanced thinking. The default for most tasks. |
| `high` | Deep thinking. Slower and uses more tokens, but produces better answers on complex problems like math proofs or debugging. |

<CodeGroup>
```python Python
response = client.chat.completions.create(
    model="zai-org-glm-4.7",
    messages=[{"role": "user", "content": "Prove that there are infinitely many primes"}],
    extra_body={"reasoning_effort": "high"}
)
```

```javascript Node.js
const response = await client.chat.completions.create({
    model: "zai-org-glm-4.7",
    messages: [{ role: "user", content: "Prove that there are infinitely many primes" }],
    reasoning_effort: "high"
});
```

```bash cURL
curl https://api.venice.ai/api/v1/chat/completions \
  -H "Authorization: Bearer $VENICE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "zai-org-glm-4.7",
    "messages": [{"role": "user", "content": "Prove that there are infinitely many primes"}],
    "reasoning_effort": "high"
  }'
```
</CodeGroup>

Works on all [supported models](#) listed above.

<Info>
Venice also accepts the format: `"reasoning": {"effort": "high"}`. Same behavior, different syntax.
</Info>

## Disabling reasoning

Skip reasoning entirely for faster, cheaper responses:

<CodeGroup>
```python Python
response = client.chat.completions.create(
    model="qwen3-4b",
    messages=[{"role": "user", "content": "What's the capital of France?"}],
    extra_body={"venice_parameters": {"disable_thinking": True}}
)
```

```javascript Node.js
const response = await client.chat.completions.create({
    model: "qwen3-4b",
    messages: [{ role: "user", content: "What's the capital of France?" }],
    venice_parameters: { disable_thinking: true }
});
```

```bash cURL
curl https://api.venice.ai/api/v1/chat/completions \
  -H "Authorization: Bearer $VENICE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-4b",
    "messages": [{"role": "user", "content": "What is the capital of France?"}],
    "venice_parameters": {"disable_thinking": true}
  }'
```
</CodeGroup>

Or use an instruct model like `qwen3-235b-a22b-instruct-2507` instead.

## Stripping thinking from responses

For models using `<think>` tags, have Venice remove them server-side:

<CodeGroup>
```python Python
response = client.chat.completions.create(
    model="qwen3-4b",
    messages=[{"role": "user", "content": "What is 15% of 240?"}],
    extra_body={"venice_parameters": {"strip_thinking_response": True}}
)
```

```javascript Node.js
const response = await client.chat.completions.create({
    model: "qwen3-4b",
    messages: [{ role: "user", content: "What is 15% of 240?" }],
    venice_parameters: { strip_thinking_response: true }
});
```

```bash cURL
curl https://api.venice.ai/api/v1/chat/completions \
  -H "Authorization: Bearer $VENICE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-4b",
    "messages": [{"role": "user", "content": "What is 15% of 240?"}],
    "venice_parameters": {"strip_thinking_response": true}
  }'
```
</CodeGroup>

Or use a model suffix: `qwen3-4b:strip_thinking_response=true`

## Parameters

| Parameter | Values | Description |
|-----------|--------|-------------|
| `reasoning_effort` | low, medium, high | Controls thinking depth |
| `reasoning.effort` | low, medium, high | Alternative format |
| `disable_thinking` | boolean | Skips reasoning entirely |
| `strip_thinking_response` | boolean | Removes `<think>` tags |

Pass `disable_thinking` and `strip_thinking_response` in `venice_parameters`, or use them as [model suffixes](/api-reference/endpoint/chat/model_feature_suffix).

## Deprecations

<Info>
`<think>` tags will eventually be deprecated across all models in favor of the `reasoning_content` field.
</Info>

For pricing and context limits, see [Current Models](/overview/models).

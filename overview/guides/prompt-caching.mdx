---
title: Prompt Caching
description: Reduce costs and latency by caching repeated prompt content
"og:title": "Prompt Caching | Venice API Docs"
"og:description": "Learn how to use prompt caching to reduce costs and latency for repeated content in your API requests."
---

Venice automatically optimizes prompt caching for supported models. No configuration is required.

When you send repeated content (like the same system prompt), the provider can serve those tokens from cache instead of reprocessing them. Cached tokens are billed at a discounted rate, and you get lower latency.

## How It Works

```
Request 1: System prompt (1000 tokens) + User message
           └─ Cached for future requests

Request 2: Same system prompt (1000 tokens) + Different user message
           └─ 1000 tokens served from cache (discounted)
```

Venice handles caching automatically by:
- Adding cache control markers to system prompts where required
- Using stable date formats to maximize cache hit rates
- Billing cached tokens at the provider's discounted rate

Caching works on prefix matching, so static content at the beginning of your prompt (like system prompts) is most likely to benefit.

## Supported Models and Pricing

Prices per 1M tokens.

| Model | Input | Cache | Output | Discount |
|-------|-------|-------|--------|----------|
| `claude-opus-45` | $6.00 | $0.60 | $30.00 | 90% |
| `openai-gpt-52` | $2.19 | $0.219 | $17.50 | 90% |
| `gemini-3-pro-preview` | $2.50 | $0.625 | $15.00 | 75% |
| `kimi-k2-thinking` | $0.75 | $0.1875 | $3.20 | 75% |
| `grok-41-fast` | $0.50 | $0.125 | $1.25 | 75% |
| `deepseek-v3.2` | $0.40 | $0.20 | $1.00 | 50% |

## Checking Cache Usage

The response includes cache statistics in the `usage` object:

```json
{
  "usage": {
    "prompt_tokens": 1500,
    "completion_tokens": 200,
    "total_tokens": 1700,
    "prompt_tokens_details": {
      "cached_tokens": 1000
    }
  }
}
```

In this example, 1000 of 1500 prompt tokens were served from cache. Using Claude Opus 4.5, you pay the cache rate for those 1000 tokens and the full input rate for the remaining 500.

## Cache Lifetime

Caches expire after a few minutes of inactivity. This works well for interactive sessions with frequent requests but won't help with sporadic traffic.

<Note>
Different models have different minimum token thresholds for caching. Some models like Claude require larger prompts (several thousand tokens) before caching activates.
</Note>

## Routing Optimization

The `prompt_cache_key` parameter improves cache hit rates by routing requests with the same key to the same backend:

```json
{
  "model": "claude-opus-45",
  "prompt_cache_key": "user-123-session-abc",
  "messages": [...]
}
```

This is useful for multi-turn conversations where you want requests to land on servers that already have your context cached.

## Maximizing Cache Hits

**Keep system prompts stable.** Avoid injecting timestamps, request IDs, or other dynamic values into your system prompt. Venice already uses stable date formats for its own system prompts.

**Use prompt_cache_key for sessions.** If you're building a multi-turn conversation, use a consistent `prompt_cache_key` (like a session ID) to improve routing.

**Check your cache hits.** Monitor `prompt_tokens_details.cached_tokens` in responses. If it's consistently zero, your prompts may be changing between requests or requests may be hitting different backend instances.
